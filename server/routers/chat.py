# server/routers/chat.py
import re
import json
import time
import asyncio
from typing import Optional, Dict, List, Set, Tuple
from collections import deque
from urllib.parse import urlparse
from collections import defaultdict

from fastapi import APIRouter, Request, Body
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field

from core.config import get_settings
from core.metrics import append_bench_cache
from core.utils import normalize_question
from evals.bench_offline import rouge_l_approx, compute_bleu, compute_evidence_score
from server.deps import get_trace_logger, new_trace_id
from core.prompts import SYSTEM_PROMPT, DEVELOPER_PROMPT, get_domain_prompt
from core.router import choose_model
from core.cache import response_cache, semantic_cache
from core.tavily import web_search_snippets, is_tavily_enabled
from core.rag import build_context
from core.quant_guard import extract_numbers, sanitize_numbers
from core.evidence import analyze_question, normalize_citations
# ì†Œë¹„ ë¶„ì„ ê¸°ëŠ¥
from core.backend_client import get_consumption_data, format_consumption_for_llm
from datetime import datetime, timedelta
# ê·¼ê±° ê¼¬ë¦¬ ë Œë”ëŸ¬ëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ import ì œê±°
# from core.answer import render_answer, render_tail

# ìµœëŒ€ 12ê°œì˜ ëŒ€í™”ë¥¼ ê¸°ë¡(ì´ì „ ëŒ€í™” ë‚´ì—­ì€ 10ê°œ) ëŠ˜ë¦¬ê¸°ë¥¼ ì›í•  ì‹œ ìµœëŒ€ 10ê°œ ê¶Œì¥
MAX_HISTORY_TURNS = 5
MAX_HISTORY_MESSAGES = MAX_HISTORY_TURNS * 2

try:
    from core.utils import count_tokens  # tiktoken ê¸°ë°˜
except Exception:
    count_tokens = lambda text, *_: max(1, len(text) // 4)

cfg = get_settings()

SMALLTALK_RE = re.compile(
    r"^\s*(ì•ˆë…•|ì•ˆë…•í•˜ì„¸ìš”|ë°˜ê°€ì›Œ|ã…+ã…‡+|í•˜ì´|hello|hi|hey|"
    r"ëˆ„êµ¬(ì•¼|ì„¸ìš”)|ìê¸°ì†Œê°œ|ë„Œ\s*ë­˜\s*ë„ì™€ì¤„|ë¬´ì—‡ì„\s*ë„ì™€ì¤„|what can you do)\b",
    re.IGNORECASE,
)
TAIL_SUPPRESS_RE = re.compile(
    r"(ì¶œì²˜|ê·¼ê±°\s*ìš”ì•½)\s*(?:ì„?\s*)?(?:ë¶™ì´|ë„£|ë‹¬|ì“°|í‘œì‹œ|í¬í•¨)\s*ì§€\s*ë§(?:ê³ |ì•„)?|"
    r"(ì¶œì²˜|ê·¼ê±°\s*ìš”ì•½)\s*ì—†[ì´ì´]|"
    r"(ì¶œì²˜|ê·¼ê±°\s*ìš”ì•½)\s*ë¹¼ê³ |"
    r"(ì¶œì²˜|ê·¼ê±°\s*ìš”ì•½)\s*ì œì™¸|"
    r"\b(no\s+sources?|without\s+(sources?|citations?|evidence))\b",
    re.IGNORECASE
)

router = APIRouter()

# ------------------------ Metrics ------------------------
_METRICS = {
    "total_requests": 0,
    "simcache_hits": 0,
    "provider_counts": {"groq": 0, "openai": 0},
    "total_prompt_tokens": 0,
    "total_completion_tokens": 0,
}
def _update_metrics(provider: str, prompt_tks: int, compl_tks: int, sim_hit: bool = False):
    _METRICS["total_requests"] += 1
    _METRICS["provider_counts"][provider] = _METRICS["provider_counts"].get(provider, 0) + 1
    _METRICS["total_prompt_tokens"] += max(0, int(prompt_tks))
    _METRICS["total_completion_tokens"] += max(0, int(compl_tks))
    if sim_hit:
        _METRICS["simcache_hits"] += 1

_MAX_RECENTS = 200
_RECENTS: deque = deque(maxlen=_MAX_RECENTS)
def _push_recent(entry: dict):
    _RECENTS.append(entry)

# ------------------------ Schema ------------------------
class ChatPayload(BaseModel):
    question: str
    context: Optional[str] = ""
    domain: Optional[str] = Field(default=None, description="apptech|ledger|invest|points|consumption")
    top_k: Optional[int] = Field(default=None, ge=1, le=10)
    max_context_chars: Optional[int] = Field(default=None, ge=200, le=8000)
    stream: Optional[bool] = Field(default=True)
    facts: Optional[Dict[str, str]] = Field(default=None, description="ì‚¬ìš©ì ì œê³µ ìˆ˜ì¹˜")
    chat_history: List[Dict[str, str]] = Field(default=[], description="ì´ì „ ëŒ€í™” ê¸°ë¡")

# ------------------------ URL / ìˆ˜ì¹˜ ê°€ë“œ ------------------------
_URL_RE = re.compile(r"https?://[^\s\])}>,]+", re.IGNORECASE)
def _normalize_host(url: str) -> Optional[str]:
    try:
        u = urlparse(url)
        if u.scheme in ("http", "https") and u.netloc:
            host = u.netloc.lower()
            if host.startswith("www."):
                host = host[4:]
            return host
    except Exception:
        return None
    return None

def _collect_allowed_hosts(passages: List[Dict], *extra_texts: str) -> Set[str]:
    hosts: Set[str] = set()
    for p in passages or []:
        for key in ("source", "url"):
            v = p.get(key)
            if isinstance(v, str):
                for u in _URL_RE.findall(v):
                    h = _normalize_host(u)
                    if h:
                        hosts.add(h)
    for t in extra_texts or []:
        if not t:
            continue
        for u in _URL_RE.findall(t):
            h = _normalize_host(u)
            if h:
                hosts.add(h)
    return hosts

def _sanitize_urls(text: str, allowed_hosts: Optional[Set[str]]) -> str:
    if not text:
        return text
    if allowed_hosts is None:
        return text
    return _URL_RE.sub(lambda m: m.group(0) if (_normalize_host(m.group(0)) in allowed_hosts) else "[ë§í¬ ìƒëµ]", text)

def _sanitize_text_with_numbers(text: str, allowed_hosts: Optional[Set[str]], allowed_numbers: set[str]) -> str:
    try:
        return sanitize_numbers(_sanitize_urls(text, allowed_hosts), allowed_numbers)
    except Exception:
        return text

# -------- ëª¨ë¸ì´ ì¨ë²„ë¦° ì¦ê±°/ê·¼ê±°ë¶€ì¡± ë¼ì¸ & inline í† í° ì œê±° --------
_BEGIN_EVD_LINE = re.compile(
    r"^\s*(?:\[?\s*ê·¼ê±°\s*ìš”ì•½\s*\]?\s*:?|"
    r"\[?\s*ì¶œì²˜\s*\]?\s*:?|"
    r"ê·¼ê±°\s*:|"
    r"\[?\s*ê·¼ê±°\s*ë¶€ì¡±\s*\]?)",
    re.IGNORECASE
)
_INADEQUATE_LINE = re.compile(r"^\s*\[\s*ê·¼ê±°\s*ë¶€ì¡±\s*\]\s*$", re.IGNORECASE)
_EVD_INLINE = re.compile(r"\[?\s*(ê·¼ê±°\s*ìš”ì•½|ì¶œì²˜|ê·¼ê±°\s*ë¶€ì¡±)\s*\]?\s*:?", re.IGNORECASE)

def cut_inline_evidence_tokens(text: str) -> str:
    if not text:
        return text
    m = _EVD_INLINE.search(text)
    if not m:
        return text.strip()
    return text[:m.start()].rstrip()

def strip_evidence_lines(text: str) -> str:
    if not text:
        return text
    lines = text.splitlines()
    out, skipping = [], False
    for ln in lines:
        if _BEGIN_EVD_LINE.match(ln):
            skipping = True
            continue
        if not skipping:
            out.append(ln)
    return "\n".join(out).rstrip()

def strip_inadequate_line(text: str) -> str:
    if not text:
        return text
    lines = [ln for ln in text.splitlines() if not _INADEQUATE_LINE.match(ln)]
    return "\n".join(lines).strip()

def sanitize_body_text(text: str) -> str:
    s = strip_inadequate_line(
        strip_evidence_lines(
            cut_inline_evidence_tokens(text)
        )
    )
    # í†µí™”/í¼ì„¼íŠ¸ ì• ë¶ˆí•„ìš” ê³µë°± ì œê±°: "103,500 ì›"â†’"103,500ì›", "58.8 %"â†’"58.8%"
    s = re.sub(r"\s+ì›\b", "ì›", s)
    s = re.sub(r"\s+%", "%", s)

    # ë¼ì¸ ë ê³µë°± ì œê±° + ì¤‘ë³µ ê³µë°± ìµœì†Œí™”
    s = "\n".join(line.rstrip() for line in s.splitlines())
    s = re.sub(r"[ \t]{2,}", " ", s)

    return s.strip()

# -------- ì§ˆì˜ì—ì„œ 'ì¶œì²˜ Nê°œ'Â·'ê³µê³µê¸°ê´€ë§Œ' ìš”êµ¬ íŒŒì‹± --------
_SRC_LIMIT_RE = re.compile(r"(?:ì¶œì²˜|source)\s*([0-9]{1,2})\s*ê°œ", re.IGNORECASE)
def _desired_source_limit(q: str) -> Optional[int]:
    m = _SRC_LIMIT_RE.search(q or "")
    if not m:
        return None
    try:
        n = int(m.group(1))
        return min(10, max(1, n))
    except Exception:
        return None

def _require_public_sources(q: str) -> bool:
    return bool(re.search(r"(ê³µê³µê¸°ê´€|ì •ë¶€|ê´€ê³„ë¶€ì²˜|ê³µê³µ|gov)", q or "", re.IGNORECASE))

def _is_public_host(host: str) -> bool:
    if not host:
        return False
    if host.endswith(".go.kr"):
        return True
    wl = set((cfg.TAVILY_DOMAINS or [])) | {"opinet.co.kr","bok.or.kr","kostat.go.kr","kosis.kr","msit.go.kr","kcc.go.kr","kca.go.kr"}
    return (host in wl) or host.endswith(".or.kr")

def _filter_sources(sources: List[Dict], limit: Optional[int], public_only: bool) -> List[Dict]:
    seen_hosts: Set[str] = set()
    picked: List[Dict] = []
    for s in sources:
        host = (s.get("site") or "").lower()
        if public_only and not _is_public_host(host):
            continue
        if host in seen_hosts:
            continue
        picked.append(s)
        seen_hosts.add(host)
        if limit and len(picked) >= limit:
            break
    if limit:
        return picked[:limit]
    return picked

# -------------- ì¶œì²˜ ë²„íŠ¼ ìƒì„± ìœ í‹¸ --------------
def _norm_button_title(t: str, max_len: int = 80) -> str:
    if not t:
        return ""
    t = re.sub(r"\s+", " ", t).strip()
    if len(t) > max_len:
        t = t[: max_len - 1].rstrip() + "â€¦"
    return t

def _ensure_http(url: str) -> str:
    if not url:
        return url
    if re.match(r"^https?://", url, re.IGNORECASE):
        return url
    return "https://" + url

def _build_source_buttons(passages_used: List[Dict], limit: Optional[int], public_only: bool) -> List[Dict]:
    """
    passages_usedë¥¼ ê¸°ë°˜ìœ¼ë¡œ dedup + host í•„í„° ë°˜ì˜í•œ ë²„íŠ¼ ëª©ë¡ ìƒì„±.
    ë°˜í™˜: [{title, url, host}]
    """
    try:
        _, uniq_sources = normalize_citations(passages_used or [], max_sources=(limit or cfg.MAX_SOURCES))
    except Exception:
        uniq_sources = []

    uniq_sources = _filter_sources(uniq_sources, limit=limit, public_only=public_only)

    buttons: List[Dict] = []
    seen_urls: Set[str] = set()

    for s in uniq_sources:
        url = _ensure_http((s.get("url") or "").strip())
        if not url:
            continue
        if url in seen_urls:
            continue
        seen_urls.add(url)
        host = _normalize_host(url) or (s.get("site") or "").lower()
        title = _norm_button_title(s.get("title") or s.get("name") or s.get("url") or host or "ì¶œì²˜")
        buttons.append({"title": title, "url": url, "host": host})

        if limit and len(buttons) >= limit:
            break

    return buttons

# ledger íŒŒìƒ ìˆ˜ì¹˜
import re as _re
WON_NUM = _re.compile(r"(?:[â‚©ï¿¦]?\s?\d{1,3}(?:,\d{3})+|\d+)\s?ì›")
def _parse_won(s: str) -> int | None:
    if not isinstance(s, str):
        return None
    m = WON_NUM.search(s)
    if not m:
        return None
    try:
        return int(_re.sub(r"[^\d]", "", m.group(0)))
    except Exception:
        return None

def _fmt_won(n: int) -> str:
    return f"{n:,}ì›"

def _derive_ledger_numbers(facts: Dict[str, str]) -> Dict[str, str]:
    out: Dict[str, str] = {}
    for key in ["êµí†µë¹„", "ì‹ë¹„"]:
        base = _parse_won(facts.get(key, "")) if facts else None
        if base is None:
            continue
        for delta in (50_000, 100_000):
            out[f"{key} í•œë„ ì œì•ˆ(-{_fmt_won(delta)})"] = _fmt_won(max(0, base - delta))
    return out

def _compose_prompts(domain: Optional[str]) -> tuple[str, str]:
    dom = get_domain_prompt(domain)
    brief_rule = "\n[ì§€ì‹œ] ë‹µë³€ì€ 2~4ë¬¸ì¥ ì´ë‚´ í•µì‹¬ ìœ„ì£¼. **ë³¸ë¬¸ë§Œ ì‘ì„±**í•˜ê³  ê·¼ê±° ì„¹ì…˜ì€ ì“°ì§€ ë§ˆì„¸ìš”."
    facts_rule = (
        "\n[ì‚¬ì‹¤ìš°ì„ ] ì•„ë˜ [ì‚¬ìš©ì ì œê³µ ìˆ˜ì¹˜]/[íŒŒìƒ ìˆ˜ì¹˜]ë§Œ ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì„¸ìš”. "
        "ì—¬ê¸°ì— ì—†ëŠ” ì¹´í…Œê³ ë¦¬Â·ë¹„ìœ¨Â·ê¸ˆì•¡ì„ ì¶”ì •í•˜ê±°ë‚˜ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”."
    )
    number_rule = (
        "\n[ìˆ˜ì¹˜ì§€ì¹¨] ì œê³µëœ ë¬¸ë§¥/ì‚¬ì‹¤(facts)ì— ì—†ëŠ” ì„ì˜ì˜ ë¹„ìœ¨(%)Â·ê¸ˆì•¡ì„ ë§Œë“¤ì§€ ë§ˆì„¸ìš”. "
        "ë¹„ìœ¨ í‘œí˜„ì´ í•„ìš”í•˜ë©´ 'ë¹„ì¤‘ì´ ë†’ìŒ/ë‚®ìŒ'ì²˜ëŸ¼ ì„œìˆ í•˜ì„¸ìš”. ì œê³µëœ ìˆ˜ì¹˜ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”."
    )
    return SYSTEM_PROMPT + brief_rule + facts_rule + number_rule + ("\n" + dom if dom else ""), DEVELOPER_PROMPT

# ------------------------ ì•ˆì „ ì›¹ ê²€ìƒ‰ ë˜í¼(ì¬ì‹œë„ í¬í•¨) ------------------------
def _safe_web_search(q: str, k: int, log) -> List[Dict]:
    """Tavily ì¥ì• (HTTP 5xx/ë„¤íŠ¸ì›Œí¬) ì‹œì—ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ í´ë°±. ê°„ë‹¨ ì¬ì‹œë„ í¬í•¨."""
    if not is_tavily_enabled():
        return []
    delay = cfg.WEB_BACKOFF_SEC
    for attempt in range(1, cfg.WEB_RETRIES + 1):
        try:
            return web_search_snippets(q, max_results=k)
        except Exception as e:
            try:
                log.warning(f"[web] tavily search failed({attempt}/{cfg.WEB_RETRIES}): {type(e).__name__}: {e}")
            except Exception:
                pass
            if attempt < cfg.WEB_RETRIES:
                try:
                    time.sleep(delay)
                except Exception:
                    pass
                delay *= 1.5
    return []

# ------------------------ Delta ê³„ì‚° í—¬í¼ (ì¤‘ë³µ ì†¡ì¶œ ë°©ì§€) ------------------------
_WS = re.compile(r"\s+")
def _collapse_ws(s: str) -> str:
    return _WS.sub("", s or "")

def _cut_idx_ws_prefix(early: str, final: str) -> int:
    """
    ê³µë°±/ê°œí–‰/ì¤‘ë³µ ìŠ¤í˜ì´ìŠ¤ë¥¼ ë¬´ì‹œí•˜ë©° earlyê°€ finalì˜ prefixì¸ì§€ ê²€ì‚¬í•˜ê³ ,
    ì›ë³¸(final)ì—ì„œ ì˜ë¼ë‚¼ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜. ì‹¤íŒ¨ ì‹œ -1.
    """
    if not early:
        return 0
    i = j = 0
    le, lf = len(early), len(final)
    while i < le and j < lf:
        ce, cf = early[i], final[j]
        if ce.isspace():
            i += 1
            continue
        if cf.isspace():
            j += 1
            continue
        if ce != cf:
            return -1
        i += 1
        j += 1
    # earlyì˜ non-space ë¬¸ìë¥¼ ëª¨ë‘ ì†Œë¹„í–ˆëŠ”ê°€?
    while i < le and early[i].isspace():
        i += 1
    if i == le:
        # jëŠ” finalì—ì„œ earlyê°€ ëë‚œ ì§í›„ì˜ ìœ„ì¹˜(ì›ë³¸ ì¸ë±ìŠ¤)
        return j
    return -1

def _map_norm_to_original_index(s: str, norm_end: int) -> int:
    """
    collapse_ws(s)ì—ì„œ norm_end ìœ„ì¹˜(ë¹„ê³µë°± ë¬¸ì ê°œìˆ˜ ê¸°ì¤€)ì— í•´ë‹¹í•˜ëŠ”
    ì›ë¬¸ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜. (ë¬¸ì ë°”ë¡œ ë’¤ index)
    """
    if norm_end <= 0:
        return 0
    count = 0
    for idx, ch in enumerate(s):
        if not ch.isspace():
            count += 1
            if count == norm_end:
                return idx + 1
    return len(s)

def _compute_delta(early_sent_parts: List[str], final_text: str, need_web: bool) -> str:
    """
    - need_web=False: íŒŒì´ë„ ë³¸ë¬¸ ì¬ì†¡ì¶œ ê¸ˆì§€(ë¹ˆ ë¬¸ìì—´ ë°˜í™˜)
    - ê·¸ ì™¸:
        1) exact startswith
        2) ê³µë°± ë¬´ì‹œ prefix ë¹„êµ
        3) early ë§ˆì§€ë§‰ 120ì suffixë¡œ final ë‚´ ìœ„ì¹˜ ê²€ìƒ‰(ì›ë¬¸/ê³µë°±ë¬´ì‹œ ëª¨ë‘)
    - ëª¨ë‘ ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ ì¬ì†¡ì¶œ ìƒëµ("")í•˜ì—¬ ì¤‘ë³µì„ 100% ì°¨ë‹¨
    """
    if not need_web:
        return ""  # ë¡œì»¬/ìŠ¤ëª°í†¡ì€ íŒŒì´ë„ ë³¸ë¬¸ ì¬ì†¡ì¶œ ê¸ˆì§€

    early_text = "".join(early_sent_parts).strip()
    if not early_text:
        return final_text  # early ìŠ¤íŠ¸ë¦¼ ì—†ìŒ â†’ íŒŒì´ë„ ì „ì²´ ì†¡ì¶œ

    # 1) exact startswith
    if final_text.startswith(early_text):
        return final_text[len(early_text):].lstrip("\n")

    # 2) ê³µë°± ë¬´ì‹œ prefix ë¹„êµ
    cut_idx = _cut_idx_ws_prefix(early_text, final_text)
    if cut_idx >= 0:
        return final_text[cut_idx:].lstrip("\n")

    # 3) suffix fallback (ë§ˆì§€ë§‰ 120ì)
    suf = early_text[-120:].strip()
    if suf:
        pos = final_text.rfind(suf)
        if pos != -1:
            return final_text[pos + len(suf):].lstrip("\n")
        # ê³µë°±ë¬´ì‹œ suffix
        f_norm = _collapse_ws(final_text)
        s_norm = _collapse_ws(suf)
        pos_norm = f_norm.rfind(s_norm)
        if pos_norm != -1:
            cut = _map_norm_to_original_index(final_text, pos_norm + len(s_norm))
            return final_text[cut:].lstrip("\n")

    # ì‹¤íŒ¨ ì‹œ ë³´ìˆ˜ì ìœ¼ë¡œ ì•„ë¬´ ê²ƒë„ ì¬ì†¡ì¶œí•˜ì§€ ì•ŠìŒ(ì¤‘ë³µ 100% ì°¨ë‹¨)
    return ""

# ------------------------ Route ------------------------
@router.post("/chat")
async def chat(req: Request):
    body = await req.json()
    payload = ChatPayload(**body)
    if isinstance(payload.stream, str):
        payload.stream = payload.stream.lower() == "true"

    question_raw = (payload.question or "").strip()
    question = normalize_question(question_raw)
    domain = (payload.domain or "").lower()
    client_ctx = payload.context or ""
    facts = payload.facts or {}
    chat_history = payload.chat_history

    if len(chat_history) > MAX_HISTORY_MESSAGES:
        chat_history = chat_history[-MAX_HISTORY_MESSAGES:]

    trace_id = new_trace_id()
    log = get_trace_logger(trace_id)

    if not question:
        return JSONResponse(status_code=400, content={
            "error": {"code": "BAD_REQUEST", "message": "question is required", "trace_id": trace_id}
        })

    # ========== ì†Œë¹„ ë¶„ì„ ê¸°ëŠ¥ (ì „ë‹¬ ë¹„êµ í¬í•¨) ==========
    consumption_keywords = ["ì†Œë¹„", "ì§€ì¶œ", "ëˆ", "ì–¼ë§ˆ", "ì“´", "spending", "expense"]
    comparison_keywords = ["ë¹„êµ", "ëŒ€ë¹„", "ì „ë‹¬", "ì§€ë‚œë‹¬", "ë³€í™”", "ì°¨ì´"]
    
    if any(kw in question for kw in consumption_keywords):
        log.info(f"[Consumption] Detected consumption question")

        # JWT í† í° ì¶”ì¶œ
        auth_header = req.headers.get("authorization", "")
        token = auth_header.replace("Bearer ", "").strip() if auth_header.startswith("Bearer ") else None

        # ë‚ ì§œ ì¶”ì¶œ (ë‘ ë‹¬ ë¹„êµ ì§€ì›)
        def extract_dates_from_question(q: str) -> tuple:
            today = datetime.now()
            
            # "Xì›”ê³¼ Yì›” ë¹„êµ" ë˜ëŠ” "Xì›” Yì›” ë¹„êµ" íŒ¨í„´ ì°¾ê¸°
            two_months = re.findall(r'(\d{1,2})ì›”', q)
            
            if len(two_months) >= 2:
                # ë‘ ë‹¬ì´ ëª…ì‹œëœ ê²½ìš°
                month1 = int(two_months[0])
                month2 = int(two_months[1])
                
                # ë” ìµœê·¼ ë‹¬ì´ current, ì´ì „ ë‹¬ì´ previous
                if month1 > month2:
                    current_month, prev_month = month1, month2
                else:
                    current_month, prev_month = month2, month1
                
                # ì—°ë„ ê³„ì‚° (ì˜¬í•´ ê¸°ì¤€)
                current_year = today.year if current_month <= today.month else today.year - 1
                prev_year = today.year if prev_month <= today.month else today.year - 1
                
                # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
                current_start = datetime(current_year, current_month, 1).strftime("%Y-%m-%d")
                if current_month == 12:
                    current_end = datetime(current_year, 12, 31).strftime("%Y-%m-%d")
                else:
                    current_end = (datetime(current_year, current_month + 1, 1) - timedelta(days=1)).strftime("%Y-%m-%d")
                
                prev_start = datetime(prev_year, prev_month, 1).strftime("%Y-%m-%d")
                if prev_month == 12:
                    prev_end = datetime(prev_year, 12, 31).strftime("%Y-%m-%d")
                else:
                    prev_end = (datetime(prev_year, prev_month + 1, 1) - timedelta(days=1)).strftime("%Y-%m-%d")
                
                return current_start, current_end, prev_start, prev_end
            
            elif len(two_months) == 1:
                # í•œ ë‹¬ë§Œ ëª…ì‹œëœ ê²½ìš°
                month = int(two_months[0])
                year = today.year if month <= today.month else today.year - 1
                start = datetime(year, month, 1)
                end = (datetime(year, month + 1, 1) - timedelta(days=1)) if month < 12 else datetime(year, 12, 31)
                return start.strftime("%Y-%m-%d"), end.strftime("%Y-%m-%d"), None, None
            
            # "ì´ë²ˆ" í‚¤ì›Œë“œ
            if "ì´ë²ˆ" in q:
                start = datetime(today.year, today.month, 1)
                return start.strftime("%Y-%m-%d"), today.strftime("%Y-%m-%d"), None, None
            
            # ê¸°ë³¸ê°’: ìµœê·¼ 30ì¼
            start = today - timedelta(days=30)
            return start.strftime("%Y-%m-%d"), today.strftime("%Y-%m-%d"), None, None

        date_result = extract_dates_from_question(question)
        
        # ë¹„êµ ìš”ì²­ ì—¬ë¶€ í™•ì¸
        is_comparison = any(kw in question for kw in comparison_keywords)
        model_name = "gpt-4o-mini"
        
        try:
            if len(date_result) == 4 and date_result[2] is not None:
                # ë‘ ë‹¬ì´ ëª…ì‹œëœ ê²½ìš° (ìë™ ë¹„êµ)
                current_start, current_end, prev_start, prev_end = date_result
                is_comparison = True
                log.info(f"[Consumption] Manual comparison: {current_start}~{current_end} vs {prev_start}~{prev_end}")
                
                # ë‘ ë‹¬ ë°ì´í„° ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
                
                
                current_data = await get_consumption_data(
                    start_date=current_start,
                    end_date=current_end,
                    token=token
                )
                
                prev_data = await get_consumption_data(
                    start_date=prev_start,
                    end_date=prev_end,
                    token=token
                )
                
                # ë°ì´í„° íŒŒì‹±
                def parse_data(data):
                    if not data or "data" not in data:
                        return {}, 0, 0
                    content = data["data"].get("content", [])
                    category_totals = defaultdict(int)
                    total = 0
                    for item in content:
                        cat = item.get("categoryName") or "ê¸°íƒ€"
                        amt = int(item.get("amount", 0))
                        category_totals[cat] += amt
                        total += amt
                    return category_totals, total, len(content)
                
                current_cats, current_total, current_count = parse_data(current_data)
                prev_cats, prev_total, prev_count = parse_data(prev_data)
                
                # ë¹„êµ í…ìŠ¤íŠ¸ ìƒì„±
                lines = [
                    f"[{current_start[:7]} vs {prev_start[:7]} ë¹„êµ]",
                    "",
                    f"**{current_start[:7]}**",
                    f"- ì´ ì§€ì¶œ: {current_total:,}ì›",
                    f"- ê±°ë˜ ê±´ìˆ˜: {current_count}ê±´",
                    "",
                    f"**{prev_start[:7]}**",
                    f"- ì´ ì§€ì¶œ: {prev_total:,}ì›",
                    f"- ê±°ë˜ ê±´ìˆ˜: {prev_count}ê±´",
                    "",
                ]
                
                # ì¦ê° ë¶„ì„
                if prev_total > 0:
                    diff = current_total - prev_total
                    diff_pct = (diff / prev_total) * 100
                    
                    if diff > 0:
                        lines.append(f"ğŸ’¡ ì´ ì§€ì¶œì´ {diff:,}ì› ì¦ê°€í–ˆì–´ìš” (+{diff_pct:.1f}%)")
                    elif diff < 0:
                        lines.append(f"ğŸ’¡ ì´ ì§€ì¶œì´ {abs(diff):,}ì› ê°ì†Œí–ˆì–´ìš” ({diff_pct:.1f}%)")
                    else:
                        lines.append("ğŸ’¡ ì´ ì§€ì¶œì´ ë™ì¼í•´ìš”")
                
                lines.append("")
                lines.append("[ì¹´í…Œê³ ë¦¬ë³„ ë³€í™”]")
                
                # ì¹´í…Œê³ ë¦¬ë³„ ì¦ê°
                all_cats = set(current_cats.keys()) | set(prev_cats.keys())
                changes = []
                
                for cat in all_cats:
                    curr = current_cats.get(cat, 0)
                    prev = prev_cats.get(cat, 0)
                    diff = curr - prev
                    
                    if prev > 0:
                        diff_pct = (diff / prev) * 100
                        changes.append((cat, diff, diff_pct, curr, prev))
                    elif curr > 0:
                        changes.append((cat, diff, 0, curr, prev))
                
                changes.sort(key=lambda x: abs(x[1]), reverse=True)
                
                for cat, diff, diff_pct, curr, prev in changes[:5]:
                    if diff > 0:
                        lines.append(f"- {cat}: {curr:,}ì› (ì´ì „ {prev:,}ì›, +{diff:,}ì› â†‘{diff_pct:.1f}%)")
                    elif diff < 0:
                        lines.append(f"- {cat}: {curr:,}ì› (ì´ì „ {prev:,}ì›, {diff:,}ì› â†“{abs(diff_pct):.1f}%)")
                    else:
                        lines.append(f"- {cat}: {curr:,}ì› (ë™ì¼)")
                
                formatted_data = "\n".join(lines)
                
            else:
                # í•œ ë‹¬ë§Œ ëª…ì‹œëœ ê²½ìš°
                start_date, end_date = date_result[0], date_result[1]
                log.info(f"[Consumption] Date range: {start_date} ~ {end_date}")
                
                if is_comparison:
                    # ì „ë‹¬ ë¹„êµ ë¶„ì„
                    from core.backend_client import get_consumption_comparison
                    formatted_data = await get_consumption_comparison(
                        current_start=start_date,
                        current_end=end_date,
                        token=token
                    )
                else:
                    # ì¼ë°˜ ì†Œë¹„ ë¶„ì„
                    data = await get_consumption_data(
                        start_date=start_date,
                        end_date=end_date,
                        token=token
                    )
                    
                    if not data or "data" not in data:
                        answer = "ì†Œë¹„ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¡œê·¸ì¸ ë˜ëŠ” ì„œë²„ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
                        formatted_data = ""
                    else:
                        formatted_data = format_consumption_for_llm(data)

            # AI ë¶„ì„ (ë¹„êµ ë°ì´í„° ë˜ëŠ” ì¼ë°˜ ë°ì´í„°)
            if formatted_data:
                analysis_prompt = f"""
ë‹¹ì‹ ì€ ê°œì¸ ì¬ì • ë¶„ì„ê°€ì´ì ì ˆì•½ ì½”ì¹˜ì…ë‹ˆë‹¤.
ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì‹¤ì œ ì†Œë¹„ ë°ì´í„°ì…ë‹ˆë‹¤.

<ì†Œë¹„ ë°ì´í„°>
{formatted_data}
</ì†Œë¹„ ë°ì´í„°>

ì§ˆë¬¸: "{question}"

ìœ„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒì„ ë¶„ì„í•´ ì£¼ì„¸ìš”.

**ì¤‘ìš”**: 
- "ì´ ì§€ì¶œ"ê³¼ "ì¹´í…Œê³ ë¦¬ë³„ ì§€ì¶œ" ì„¹ì…˜ì˜ ê¸ˆì•¡ì„ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”
- "ìµœê·¼ ê±°ë˜" ì„¹ì…˜ì€ ê°œë³„ ê±°ë˜ ë‚´ì—­ì¼ ë¿, ì¹´í…Œê³ ë¦¬ ì´ì•¡ì´ ì•„ë‹™ë‹ˆë‹¤
- ì˜ˆ: ì‹ë¹„ ì´ì•¡ì€ 108,300ì›ì´ì§€, ìµœê·¼ ê±°ë˜ì˜ 16,800ì›ì´ ì•„ë‹™ë‹ˆë‹¤
- ë¹„êµ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì¦ê° íŒ¨í„´ì„ ì •í™•íˆ ì–¸ê¸‰í•˜ì„¸ìš”

ë¶„ì„ ë‚´ìš©:
1. ì‚¬ìš©ìê°€ ê°€ì¥ ë§ì´ ì“´ ì§€ì¶œ ì¹´í…Œê³ ë¦¬ì™€ ê¸ˆì•¡ (ì •í™•í•œ ìˆ«ì ì‚¬ìš©)
2. {"ë¹„êµ ë¶„ì„ ë° ë³€í™” ì´ìœ " if is_comparison else "í•´ë‹¹ í•­ëª©ì´ ë†’ì€ ì´ìœ  ì¶”ì •"}
3. ì ˆì•½ì„ ìœ„í•œ êµ¬ì²´ì ì¸ ì œì•ˆ 2~3ê°€ì§€

ì¶œë ¥ì€ ê°„ê²°í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ í•´ì£¼ì„¸ìš”.
"""
                
                choice = choose_model(question, formatted_data)
                provider_name, model_name, model_client = choice.name, choice.model, choice.client
                result = await model_client.generate(
                    analysis_prompt, "", "", "", chat_history=[]
                )
                
                answer = result.strip()
            else:
                answer = "ì†Œë¹„ ë°ì´í„°ë¥¼ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (SSE)
            if payload.stream:
                async def consumption_stream(model_name_local: str):
                    for ch in answer:
                        yield f"data: {ch}\n\n"

                    meta = {
                        "provider": "consumption_analysis",
                        "model": model_name,
                        "usage": {
                            "prompt_tokens": 0,
                            "completion_tokens": len(answer),
                            "total_tokens": len(answer)
                        },
                        "source_buttons": [],
                        "chat_history": []
                    }
                    yield f"event: done\ndata: {json.dumps(meta, ensure_ascii=False)}\n\n"
                    yield "event: end\ndata: [DONE]\n\n"

                return StreamingResponse(
                    consumption_stream(model_name),
                    media_type="text/event-stream",
                    headers={
                        "Cache-Control": "no-cache",
                        "Connection": "keep-alive",
                        "X-Accel-Buffering": "no"
                    }
                )
            else:
                return {"answer": answer, "chat_history": chat_history}

        except Exception as e:
            log.error(f"[Consumption] Failed: {e}")
            err = f"ì†Œë¹„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            if payload.stream:
                async def err_stream():
                    for ch in err:
                        yield f"data: {ch}\n\n"
                    yield f"event: done\ndata: {{}}\n\n"
                    yield "event: end\ndata: [DONE]\n\n"
                return StreamingResponse(err_stream(), media_type="text/event-stream")
            return {"answer": err}
    # ========== ì†Œë¹„ ë¶„ì„ ë ==========


    # facts â†’ context
    if facts:
        client_ctx += "\n\n[ì‚¬ìš©ì ì œê³µ ìˆ˜ì¹˜]\n" + "\n".join([f"- {k}: {v}" for k, v in facts.items()])
    if domain == "ledger" and facts:
        d = _derive_ledger_numbers(facts)
        if d:
            client_ctx += "\n\n[íŒŒìƒ ìˆ˜ì¹˜]\n" + "\n".join([f"- {k}: {v}" for k, v in d.items()])
    if domain == "consumption" and facts:
        client_ctx += f"\n\n[ì†Œë¹„ ìš”ì•½]\nì´ë²ˆ ë‹¬ ì†Œë¹„ ìš”ì•½: " + ", ".join([f"{k}: {v}" for k, v in facts.items()])

    # ---------------- Grounding policy ----------------
    need_web, news_like, explicit_src = analyze_question(question_raw)
    suppress_tail = True  # ë³¸ë¬¸ ê¼¬ë¦¬(ê·¼ê±°/ì¶œì²˜) ì™„ì „ ì–µì œ
    is_smalltalk = bool(SMALLTALK_RE.match(question_raw))
    if domain in {"ledger", "consumption", "invest", "points", "apptech"} and not (news_like or explicit_src):
        need_web = False

    # (evd_minì€ í˜„ì¬ ë‚´ë¶€ ì‚¬ìš© X, í˜¸í™˜ ìœ ì§€)
    evd_min = cfg.EVIDENCE_MIN
    if news_like and getattr(cfg, "EVIDENCE_MIN_NEWS", None) is not None:
        evd_min = max(evd_min, cfg.EVIDENCE_MIN_NEWS)
    elif news_like:
        evd_min = max(evd_min, 0.50)
    if explicit_src:
        evd_min = max(evd_min, 0.45)

    desired_limit = _desired_source_limit(question_raw)
    public_only = _require_public_sources(question_raw)

    # ---------------- Semantic cache fast-path ----------------
    try:
        cached = await semantic_cache.get(question, facts)
    except Exception:
        cached = None

    if cached:
        used_passages_cached = cached.get("sources", [])
        ctx_cached = cached.get("context_used", "") or ""
        allowed_hosts_cached = _collect_allowed_hosts(used_passages_cached, ctx_cached, client_ctx)
        allowed_numbers_cached = extract_numbers(ctx_cached + " " + " ".join([s.get("text", "") for s in used_passages_cached]) + " " + client_ctx)
        answer_cached = _sanitize_text_with_numbers(cached.get("answer", ""), allowed_hosts_cached, allowed_numbers_cached)
        answer_cached = sanitize_body_text(answer_cached)
        model_cached = (cached.get("model") or "-")
        provider_cached = (cached.get("provider") or "openai")
        usage_cached = cached.get("usage") or {
            "prompt_tokens": count_tokens((SYSTEM_PROMPT or "") + (DEVELOPER_PROMPT or "") + question + ctx_cached, model_cached),
            "completion_tokens": count_tokens(answer_cached, model_cached),
            "total_tokens": count_tokens((SYSTEM_PROMPT or "") + (DEVELOPER_PROMPT or "") + question + ctx_cached, model_cached) + count_tokens(answer_cached, model_cached),
        }
        updated_history_cached = chat_history + [
            {"role": "user", "content": question},
            {"role": "assistant", "content": answer_cached}
        ]

        # ë²„íŠ¼ ìƒì„± (ìºì‹œ ê²°ê³¼ ê¸°ë°˜)
        try:
            source_buttons_cached = _build_source_buttons(used_passages_cached, desired_limit, public_only)
        except Exception:
            source_buttons_cached = []

        if payload.stream:
            async def event_stream_cache():
                yield f"event: start\ndata: {trace_id}\n\n"
                for line in (answer_cached or "").splitlines():
                    if line.strip():
                        yield f"data: {line}\n\n"
                meta = {
                    "provider": provider_cached, "model": model_cached,
                    "usage": usage_cached, "context_used": ctx_cached,
                    "sources": used_passages_cached, "source_buttons": source_buttons_cached,
                    "cost_usd_estimate": None,
                    "chat_history": updated_history_cached,
                    "from_cache": True,
                }
                yield f"event: done\ndata: {json.dumps(meta, ensure_ascii=False)}\n\n"
                yield "event: end\ndata: [DONE]\n\n"
            return StreamingResponse(event_stream_cache(), media_type="text/event-stream")

        return JSONResponse(content={
            "trace_id": trace_id,
            "answer": answer_cached,
            "context_used": ctx_cached,
            "sources": used_passages_cached,
            "source_buttons": source_buttons_cached,
            "model": model_cached,
            "provider": provider_cached,
            "usage": usage_cached,
            "chat_history": updated_history_cached,
            "from_cache": True,
        })

    # ---------------- Retrieval ----------------
    t_retr_s = time.monotonic()
    brief_mode = (len(question) < 40) and (not news_like) and (not explicit_src)

    initial = [{"text": client_ctx, "source": "client", "title": "client"}] if client_ctx else []

    if (not need_web and brief_mode) or is_smalltalk:
        ctx, used_passages = (client_ctx, initial)
    else:
        initial_k = cfg.NEWS_MIN_K if (news_like or explicit_src) else 2
        web_snips = _safe_web_search(question_raw, initial_k, log) if need_web else []
        ctx, used_passages = build_context(
            question_raw,
            web_snippets=(initial + web_snips),
            top_k=payload.top_k,
            max_context_chars=payload.max_context_chars,
            min_k=(initial_k if (news_like or explicit_src) else None),
            prefer_web_first=(news_like or explicit_src)
        )
    retrieval_ms = round((time.monotonic() - t_retr_s) * 1000, 1)

    # Guards allow-list
    allowed_hosts = _collect_allowed_hosts(used_passages, ctx or "", client_ctx)
    allowed_numbers = extract_numbers((ctx or "") + " " + " ".join([s.get("text", "") for s in used_passages]) + " " + client_ctx)

    # Routing (GPT only)
    choice = choose_model(question, ctx)
    provider_name, model_name, model_client = choice.name, choice.model, choice.client
    sys_p, dev_p = _compose_prompts(domain)

    # ------------------------ Streaming ------------------------
    if payload.stream:
        async def event_stream():
            t_gen_s = time.monotonic()
            full_chunks: List[str] = []
            did_finalize = False

            # need_web=True(ë‰´ìŠ¤/ê·¼ê±°ìš”ì²­)ë©´ ì´ˆì•ˆì€ ìŠ¤íŠ¸ë¦¼í•˜ì§€ ì•Šê³  íŒŒì´ë„ë§Œ ì†¡ì¶œ
            stream_early = not (need_web and not is_smalltalk)

            # ì´ˆë°˜ì— ë³´ë‚¸ ë³¸ë¬¸ ëˆ„ì  â†’ íŒŒì´ë„ ì¤‘ë³µ ë°©ì§€
            sent_parts: List[str] = []

            def _usage_obj(full_text: str) -> Dict:
                p = count_tokens(sys_p + dev_p + question + (ctx or ""), model_name)
                c = max(1, len(full_text) // 4)
                return {"prompt_tokens": p, "completion_tokens": c, "total_tokens": p + c}

            async def _finalize_and_get_meta(full_text: str) -> Tuple[str, Dict, str]:
                nonlocal allowed_hosts, allowed_numbers

                final_text = sanitize_body_text(full_text)
                ctx_used = ctx
                passages_used = used_passages

                # í’ˆì§ˆ ë³´ê°• ì¬ê²€ìƒ‰(ë³¸ë¬¸ë§Œ): tail ë¶€ì°©ê³¼ ë¬´ê´€í•˜ê²Œ ìˆ˜í–‰
                if need_web and not is_smalltalk:
                    try:
                        evd0 = compute_evidence_score(ctx_used, final_text)
                    except Exception:
                        evd0 = 0.0
                    try:
                        web3 = _safe_web_search(question_raw, cfg.WEB_MAX_RESULTS, log)
                        if web3:
                            initial3 = [{"text": client_ctx, "source": "client", "title": "client"}] if client_ctx else []
                            ctx3, used3 = build_context(
                                question_raw,
                                web_snippets=(initial3 + web3),
                                top_k=cfg.TOP_K,
                                max_context_chars=cfg.MAX_CONTEXT_CHARS,
                                min_k=cfg.NEWS_MIN_K,
                                prefer_web_first=True
                            )
                            safer = await model_client.generate(sys_p, dev_p, question, ctx3, chat_history=chat_history)
                            safer = sanitize_body_text(_sanitize_text_with_numbers(safer, _collect_allowed_hosts(used3, ctx3), extract_numbers(ctx3)))
                            _, s3 = normalize_citations(used3, max_sources=cfg.MAX_SOURCES)
                            if (compute_evidence_score(ctx3, safer) >= evd0) or (len(s3) > 0):
                                final_text = safer
                                allowed_hosts = _collect_allowed_hosts(used3, ctx3)
                                allowed_numbers = extract_numbers(ctx3)
                                ctx_used = ctx3
                                passages_used = used3
                    except Exception:
                        pass

                # âœ… ë³¸ë¬¸ ê¼¬ë¦¬([ì¶œì²˜]/[ê·¼ê±° ìš”ì•½])ëŠ” ë” ì´ìƒ ë¶™ì´ì§€ ì•ŠìŒ

                # ì´ë¯¸ ë³´ë‚¸ ë³¸ë¬¸(early)ì„ ì œì™¸í•œ ì°¨ì´ë§Œ ì†¡ì¶œ
                resend = _compute_delta(sent_parts if stream_early else [], final_text, need_web)

                generation_ms = round((time.monotonic() - t_gen_s) * 1000, 1)
                usage = _usage_obj(final_text)

                # ë²„íŠ¼ ìƒì„±
                try:
                    source_buttons = _build_source_buttons(passages_used, desired_limit, public_only)
                except Exception:
                    source_buttons = []

                # ë©”íŠ¸ë¦­/ë²¤ì¹˜ ê¸°ë¡ì€ ì‹¤ì œ ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ ê¸°ì¤€
                try:
                    _update_metrics(provider_name, usage["prompt_tokens"], usage["completion_tokens"], sim_hit=False)
                    _push_recent({
                        "provider": provider_name, "model": model_name,
                        "total_ms": retrieval_ms + generation_ms,
                        "retrieval_ms": retrieval_ms, "generation_ms": generation_ms,
                        "tokens_in": usage["prompt_tokens"], "tokens_out": usage["completion_tokens"],
                        "q": question, "trace_id": trace_id,
                    })
                except Exception:
                    pass

                try:
                    append_bench_cache({
                        "q": question, "answer": final_text,
                        "rougeL": round(rouge_l_approx("", final_text), 3),
                        "bleu": round(compute_bleu("", final_text), 3),
                        "evidence_score": round(compute_evidence_score(ctx_used, final_text), 3),
                        "provider": provider_name, "model": model_name,
                        "total_ms": retrieval_ms + generation_ms,
                        "retrieval_ms": retrieval_ms, "generation_ms": generation_ms,
                        "tokens_in": usage["prompt_tokens"], "tokens_out": usage["completion_tokens"],
                    })
                except Exception:
                    pass

                try:
                    cost = (usage["total_tokens"] / 1000) * (0.003)
                except Exception:
                    cost = None
                updated_history = chat_history + [
                    {"role": "user", "content": question_raw},
                    {"role": "assistant", "content": final_text}
                ]
                meta = {
                    "provider": provider_name, "model": model_name,
                    "usage": usage, "context_used": ctx_used, "sources": passages_used,
                    "source_buttons": source_buttons,
                    "cost_usd_estimate": round(cost, 6) if cost is not None else None,
                    "chat_history": updated_history,
                    "from_cache": False,
                }

                try:
                    await semantic_cache.set(question, {
                        "answer": final_text, "provider": provider_name, "model": model_name,
                        "sources": passages_used, "context_used": ctx_used, "usage": usage,
                    }, facts=facts)
                except Exception:
                    pass

                return resend, meta, final_text

            try:
                yield f"event: start\ndata: {trace_id}\n\n"
                if stream_early:
                    buf = ""
                    async for chunk in model_client.stream(sys_p, dev_p, question, ctx, chat_history=chat_history):
                        if not chunk:
                            continue
                        full_chunks.append(chunk)
                        buf += chunk
                        while "\n" in buf:
                            line, buf = buf.split("\n", 1)
                            line = line.strip()
                            if not line:
                                continue
                            line = cut_inline_evidence_tokens(line)
                            if not line:
                                continue
                            try:
                                safe = _sanitize_text_with_numbers(line, allowed_hosts, allowed_numbers)
                            except Exception:
                                safe = line
                            yield f"data: {safe}\n\n"
                            sent_parts.append(safe + "\n")
                    if buf.strip():
                        tail_line = cut_inline_evidence_tokens(buf.strip())
                        if tail_line:
                            try:
                                safe_tail = _sanitize_text_with_numbers(tail_line, allowed_hosts, allowed_numbers)
                            except Exception:
                                safe_tail = tail_line
                            yield f"data: {safe_tail}\n\n"
                            sent_parts.append(safe_tail + "\n")
                else:
                    async for chunk in model_client.stream(sys_p, dev_p, question, ctx, chat_history=chat_history):
                        if not chunk:
                            continue
                        full_chunks.append(chunk)

                did_finalize = True
                resend, meta, final_text_all = await _finalize_and_get_meta("".join(full_chunks))
                if resend:
                    for ln in resend.splitlines():
                        if ln.strip():
                            yield f"data: {ln}\n\n"
                yield f"event: done\ndata: {json.dumps(meta, ensure_ascii=False)}\n\n"
                yield "event: end\ndata: [DONE]\n\n"

            except (asyncio.CancelledError, GeneratorExit):
                try:
                    if not did_finalize:
                        await _finalize_and_get_meta("".join(full_chunks))
                except Exception:
                    pass
                raise
            except Exception as e:
                try:
                    yield f"event: error\ndata: {str(e)}\n\n"
                except Exception:
                    pass

        return StreamingResponse(event_stream(), media_type="text/event-stream")

    # ------------------------ JSON ------------------------
    t_gen_s = time.monotonic()
    answer = await model_client.generate(sys_p, dev_p, question, ctx, chat_history=chat_history)
    answer = sanitize_body_text(answer)

    try:
        # í’ˆì§ˆ ë³´ê°• ì¬ê²€ìƒ‰(ë³¸ë¬¸ë§Œ)
        if need_web and not is_smalltalk:
            evd0 = compute_evidence_score(ctx, answer)

            webx = _safe_web_search(question_raw, cfg.WEB_MAX_RESULTS, log)
            if webx:
                initialx = [{"text": client_ctx, "source": "client", "title": "client"}] if client_ctx else []
                ctxx, usedx = build_context(
                    question_raw,
                    web_snippets=(initialx + webx),
                    top_k=payload.top_k,
                    max_context_chars=payload.max_context_chars,
                    min_k=cfg.NEWS_MIN_K,
                    prefer_web_first=True
                )
                ans2 = await model_client.generate(sys_p, dev_p, question, ctxx, chat_history=chat_history)
                ans2 = sanitize_body_text(ans2)
                if compute_evidence_score(ctxx, ans2) >= evd0:
                    answer, used_passages, ctx = ans2, usedx, ctxx

            # âœ… ë³¸ë¬¸ ê¼¬ë¦¬([ì¶œì²˜]/[ê·¼ê±° ìš”ì•½])ëŠ” ë¶™ì´ì§€ ì•ŠìŒ
    except Exception:
        pass

    generation_ms = round((time.monotonic() - t_gen_s) * 1000, 1)
    allowed_hosts = _collect_allowed_hosts(used_passages, ctx or "", client_ctx)
    allowed_numbers = extract_numbers((ctx or "") + " " + " ".join([s.get("text", "") for s in used_passages]) + " " + client_ctx)
    answer = _sanitize_text_with_numbers(answer, allowed_hosts, allowed_numbers)

    # ë²„íŠ¼ ìƒì„±
    try:
        source_buttons_final = _build_source_buttons(used_passages, desired_limit, public_only)
    except Exception:
        source_buttons_final = []

    usage = {
        "prompt_tokens": count_tokens(sys_p + dev_p + question + (ctx or ""), model_name),
        "completion_tokens": count_tokens(answer, model_name),
    }
    usage["total_tokens"] = usage["prompt_tokens"] + usage["completion_tokens"]

    try:
        _update_metrics(provider_name, usage["prompt_tokens"], usage["completion_tokens"], sim_hit=False)
        _push_recent({
            "provider": provider_name, "model": model_name,
            "total_ms": retrieval_ms + generation_ms,
            "retrieval_ms": retrieval_ms, "generation_ms": generation_ms,
            "tokens_in": usage["prompt_tokens"], "tokens_out": usage["completion_tokens"],
            "q": question, "trace_id": trace_id,
        })
    except Exception:
        pass

    try:
        append_bench_cache({
            "q": question, "answer": answer,
            "rougeL": round(rouge_l_approx("", answer), 3),
            "bleu": round(compute_bleu("", answer), 3),
            "evidence_score": round(compute_evidence_score(ctx, answer), 3),
            "provider": provider_name, "model": model_name,
            "total_ms": retrieval_ms + generation_ms,
            "retrieval_ms": retrieval_ms, "generation_ms": generation_ms,
            "tokens_in": usage["prompt_tokens"], "tokens_out": usage["completion_tokens"],
        })
    except Exception:
        pass

    try:
        await semantic_cache.set(question, {
            "answer": answer, "provider": provider_name, "model": model_name,
            "sources": used_passages, "context_used": ctx, "usage": usage,
        }, facts=facts)
    except Exception:
        pass

    updated_history = chat_history + [
        {"role": "user", "content": question_raw},
        {"role": "assistant", "content": answer}
    ]

    return JSONResponse(content={
        "trace_id": trace_id,
        "answer": answer,
        "context_used": ctx,
        "sources": used_passages,
        "source_buttons": source_buttons_final,
        "model": model_name,
        "provider": provider_name,
        "usage": usage,
        "chat_history": updated_history
    })

# ------------------------ Utils ------------------------
@router.get("/chat/health")
async def health():
    return {"ok": True}

@router.get("/metrics")
async def metrics():
    total = max(1, _METRICS["total_requests"])
    return {
        "total_requests": _METRICS["total_requests"],
        "simcache_hits": _METRICS["simcache_hits"],
        "hit_rate": round(_METRICS["simcache_hits"] / total, 4),
        "provider_counts": _METRICS["provider_counts"],
        "avg_prompt_tokens": round(_METRICS["total_prompt_tokens"] / total, 2),
        "avg_completion_tokens": round(_METRICS["total_completion_tokens"] / total, 2),
    }